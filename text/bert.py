# -*- coding: utf-8 -*-
"""bert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gauyg2rx0ORD33dNWPdmspro185zLVL6
"""

import pickle
emoticons = {
    # Happy faces
    tuple([":-))", ":))", ":-)", ":)", ":-]", ":]", ":->", ":>", "8-)", "8)", ":-}", ":}", ":^)", "=]", "=)"]):
        "Smiley",

    # Laughing
    tuple([":-D", ":D", "8-D", "8D", "=D", "=3", "B^D", "c:", "C:", "x-D", "xD", "X-D", "XD"]):
        "Laughing",

    # Crying/Happy tears
    tuple([":'-(", ":'(", ":=("]): "Crying",
    tuple([":'-)", ":')", ":\"D"]): "Happy",

    # Sad/Frown
    tuple([":-(", ":(", ":-c", ":c", ":-<", ":<", ":-[", ":[", ":-||", ":{", ":@", ";("]):
        "Frown",

    # Angry
    tuple([">:(", ">:["]): "Angry",

    # Horror
    tuple(["D-':", "D:<", "D:", "D8", "D;", "D=", "DX"]):
        "Horror",

    # Surprise
    tuple([":-O", ":O", ":-o", ":o", ":-0", ":0", "8-0", ">:O", "=O", "=o", "=0"]):
        "Surprise",

    # Cat faces
    tuple([":-3", ":3", "=3", "x3", "X3"]): "Cat",
    tuple([">:3"]): "Lion",

    # Kiss
    tuple([":-*", ":*", ":x"]): "Kiss",

    # Wink
    tuple([";-)", ";)", "*-)", "*)", ";-]", ";]", ";^)", ";>", ":-,", ";D", ";3"]):
        "Wink",

    # Tongue
    tuple([":-P", ":P", "X-P", "XP", "x-p", ":p", ":b", "d:", "=p", ">:P", ":-J"]):
        "Tongue",

    # Skeptical
    tuple([":-/", ":/", ":-.", ">=\\", ">:/", ":\\", "=/", "=\\", ":L", "=L", ":S", "',:-|", "',:-l"]):
        "Skeptical",

    # Straight face
    tuple([":-|", ":|"]): "Straight",

    # Embarrassed
    tuple([":$", "://)", "://3"]): "Embarrassed",

    # Sealed lips
    tuple([":-X", ":X", ":-#", ":#", ":-&", ":&"]): "Sealed",

    # Angel
    tuple(["O:-)", "O:)", "0:-3", "0:3", "0:-)", "0:)", "0;^)"]):
        "Angel",

    # Evil
    tuple([">:-)", ">:)", "}:-)", "}:)", "3:-)", "3:)", ">;-)", ">;)"]):
        "Evil",

    # Cool/Bored
    tuple(["|;-)", "|-O", "B-)"]): "Cool",

    # Drunk
    tuple(["%-)", "%"]): "Drunk",

    # Sick
    tuple([":-###..", ":###.."]): "Sick",

    # Dumb
    tuple(["<:-|"]): "Dumb",

    # Grimacing
    tuple([":E"]): "Grimacing",

    # Skull
    tuple(["8-X", "8=X", "x-3", "x=3"]): "Skull",

    # Chicken
    tuple(["~:>"]): "Chicken"
}

# Configure logging
import logging
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from scipy.stats import pearsonr
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import KFold
from sklearn.preprocessing import MinMaxScaler
import keras
from keras import regularizers
from keras import layers, Model, callbacks, optimizers
from transformers import AutoTokenizer
from google.colab import drive
import re
import unicodedata
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import nltk
nltk.download(['punkt', 'punkt_tab', 'stopwords', 'wordnet'], quiet=True)
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Constants and Hyperparameters
SEQUENCE_LENGTH = 200
TRAIT_NAMES = ['ope', 'con', 'ext', 'agr', 'neu']
EMBEDDING_DIM = 128
LSTM_UNITS = 64
DENSE_UNITS = 64
BATCH_SIZE = 64
EPOCHS = 15
NUM_FOLDS = 5
RANDOM_STATE = 42
DROPOUT_RATE = 0.2
LEARNING_RATE = 3e-5
REDUCE_LR_FACTOR = 0.1
REDUCE_LR_PATIENCE = 3
MIN_LR = 1e-7
SCALE_CON_MIN = 1.25
SCALE_CON_RANGE = 5.0
SCALE_OTHER_MIN = 1.0
SCALE_OTHER_RANGE = 5.0

def clean_text(text: str) -> str:
    # text = unicodedata.normalize('NFD', text)
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    # filter out emoticons
    for emoticon, label in emoticons.items():
        text = re.sub('|'.join(map(re.escape, emoticon)), f' {label} ', text)

    text = re.sub(r'[^a-zA-Z\s]', ' ', text, flags=re.MULTILINE)
    text = re.sub(r'(\w)(\1{2,})', lambda m: m.group(1) + m.group(1), text, flags=re.MULTILINE)
    text = re.sub(r'\s+', ' ', text)

    words = [lemmatizer.lemmatize(word) for word in word_tokenize(text) if word.lower() not in stop_words]

    return ' '.join(words).lower()


def load_and_preprocess(cleaned_path: str, liwc_path: str):
    data = pd.read_csv(cleaned_path)
    liwc_profile = pd.read_csv(liwc_path)
    data['text'] = data['text'].fillna("").apply(clean_text).astype(str)

    # Tokenize text
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    tokens = tokenizer(
        data['text'].tolist(),
        padding='max_length',
        truncation=True,
        max_length=SEQUENCE_LENGTH,
        return_tensors='np'
    )['input_ids']

    linguistic_data = liwc_profile.iloc[:, 1:]
    linguistic_data = linguistic_data.drop(columns=['Seg'])
    scaler = MinMaxScaler()
    scaled_ling = scaler.fit_transform(linguistic_data)

    y = data[TRAIT_NAMES].values
    y_scaled = np.zeros_like(y, dtype=np.float32)
    scaling_factors = {}
    for i, trait in enumerate(TRAIT_NAMES):
        if trait == 'con':
            y_scaled[:, i] = (y[:, i] - SCALE_CON_MIN) / SCALE_CON_RANGE
            scaling_factors[trait] = {'min': SCALE_CON_MIN, 'scale': SCALE_CON_RANGE}
        else:
            y_scaled[:, i] = (y[:, i] - SCALE_OTHER_MIN) / SCALE_OTHER_RANGE
            scaling_factors[trait] = {'min': SCALE_OTHER_MIN, 'scale': SCALE_OTHER_RANGE}

    return tokens, scaled_ling, y_scaled, scaling_factors


def build_model(vocab_size: int, n_linguistic_features: int) -> Model:

    # Text input branch
    text_input = layers.Input(shape=(SEQUENCE_LENGTH,), name='text_input')
    x = layers.Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM)(text_input)
    x = layers.Bidirectional(layers.LSTM(LSTM_UNITS, return_sequences=True))(x)
    x = layers.GlobalMaxPooling1D()(x)
    x = layers.Dense(DENSE_UNITS, activation='relu', kernel_regularizer=regularizers.l2(0.25))(x)
    x = layers.Dropout(DROPOUT_RATE)(x)

    # Linguistic features branch
    ling_input = layers.Input(shape=(n_linguistic_features,), name='ling_input')
    y = layers.BatchNormalization()(ling_input)
    y = layers.Dense(DENSE_UNITS, activation='relu')(y)
    y = layers.Dropout(DROPOUT_RATE)(y)

    # Combine branches
    combined = layers.concatenate([x, y])
    combined = layers.Dense(DENSE_UNITS * 2, activation='relu')(combined)
    combined = layers.Dropout(DROPOUT_RATE)(combined)
    combined = layers.Dense(DENSE_UNITS, activation='relu')(combined)

    # Output layers for each trait
    outputs = [layers.Dense(1, activation='linear', name=trait)(combined) for trait in TRAIT_NAMES]

    # Define the model
    model = Model(inputs=[text_input, ling_input], outputs=outputs)

    # Define metrics using string identifiers for brevity
    metrics = {
        trait: ['RootMeanSquaredError', 'MeanSquaredError', 'MeanAbsoluteError']
        for trait in TRAIT_NAMES
    }
    initial_learning_rate = LEARNING_RATE
    lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(
        initial_learning_rate,
        decay_steps=10000,  # Adjust based on your data size
        end_learning_rate=0.0,
        power=1.0
    )
    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)

    # Compile the model
    model.compile(
    optimizer=optimizer,  # Use the learning rate schedule
    loss='mse',
    metrics=metrics
)

    # Logging
    logging.info(f'Metrics: {metrics}')
    logging.info('Model built successfully.')
    model.summary(print_fn=lambda x: logging.info(x))
    with open('trained_model.pkl', 'wb') as file: # Step 2: Choose filename
            pickle.dump(model, file)
    return model



def train_and_evaluate(tokens, scaled_ling, y_scaled, scaling_factors):
    kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=RANDOM_STATE)
    fold_metrics = {trait: {'r2': [], 'pearson': [], 'mae': [], 'rmse': []} for trait in TRAIT_NAMES}

    for fold, (train_idx, val_idx) in enumerate(kf.split(tokens)):
        logging.info(f"\nFold {fold + 1}/{NUM_FOLDS}")

        # Build and compile model
        model = build_model(
            vocab_size=AutoTokenizer.from_pretrained("bert-base-uncased").vocab_size,
            n_linguistic_features=scaled_ling.shape[1]
        )

        # Early stopping to prevent overfitting
        # early_stop_loss = callbacks.EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)

        # Reduce learning rate for stable convergence
        reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=REDUCE_LR_FACTOR, patience=REDUCE_LR_PATIENCE, min_lr=MIN_LR)

        # Save best model per fold
        checkpoint = callbacks.ModelCheckpoint(
            f'model_fold_{fold + 1}.keras', save_best_only=True, monitor='val_loss', mode='min'
        )

        callback_list = [reduce_lr, checkpoint]

        # Train model
        model.fit(
            [tokens[train_idx], scaled_ling[train_idx]],
            [y_scaled[train_idx, i] for i in range(y_scaled.shape[1])],
            validation_data=(
                [tokens[val_idx], scaled_ling[val_idx]],
                [y_scaled[val_idx, i] for i in range(y_scaled.shape[1])]
            ),
            epochs=EPOCHS,
            batch_size=BATCH_SIZE,
            callbacks=callback_list,
            verbose=1
        )

        # Load best weights and evaluate
        model.load_weights(f'model_fold_{fold + 1}.keras')
        predictions = np.array(model.predict([tokens[val_idx], scaled_ling[val_idx]])).squeeze().T

        # Rescale predictions and ground truth
        predictions_rescaled = np.zeros_like(predictions)
        actual_rescaled = np.zeros_like(predictions)
        for i, trait in enumerate(TRAIT_NAMES):
            predictions_rescaled[:, i] = predictions[:, i] * scaling_factors[trait]['scale'] + scaling_factors[trait]['min']
            actual_rescaled[:, i] = y_scaled[val_idx, i] * scaling_factors[trait]['scale'] + scaling_factors[trait]['min']

        # Calculate and log metrics
        for i, trait in enumerate(TRAIT_NAMES):
            pred, act = predictions_rescaled[:, i], actual_rescaled[:, i]
            r2 = r2_score(act, pred)
            mae = mean_absolute_error(act, pred)
            rmse = np.sqrt(mean_squared_error(act, pred))
            pearson_corr, _ = pearsonr(act, pred)

            fold_metrics[trait]['r2'].append(r2)
            fold_metrics[trait]['mae'].append(mae)
            fold_metrics[trait]['pearson'].append(pearson_corr)
            fold_metrics[trait]['rmse'].append(rmse)

            logging.info(f"{trait.upper()} | "
                         f"R²: {r2:.4f} | "
                         f"Pearson Corr: {pearson_corr:.4f} | "
                         f"MAE: {mae:.4f} | "
                         f"RMSE: {rmse:.4f}")
        print_metrics(fold_metrics)

    return fold_metrics


def print_metrics(fold_metrics):
    print("\nAggragate Metrics:")
    for trait in TRAIT_NAMES:
        metrics = fold_metrics[trait]
        r2_mean = np.mean(metrics['r2'])
        r2_std = np.std(metrics['r2'])
        pearson_mean = np.mean(metrics['pearson'])
        pearson_std = np.std(metrics['pearson'])
        mae_mean = np.mean(metrics['mae'])
        mae_std = np.std(metrics['mae'])
        rmse_mean = np.mean(metrics['rmse'])
        rmse_std = np.std(metrics['rmse'])

        print(f"{trait.upper()} | "
                     f"MAE: {mae_mean:.4f} ± {mae_std:.4f} | "
                     f"RMSE: {rmse_mean:.4f} ± {rmse_std:.4f} | "
                     f"Corr: {pearson_mean:.4f} ± {pearson_std:.4f} | "
                     f"R²: {r2_mean:.4f} ± {r2_std:.4f}")

def predict_personality(df: pd.DataFrame, model_path: str = "trained_model.pkl") -> pd.DataFrame:
    """Predicts personality traits based on text and LIWC features.

    Args:
        df (pd.DataFrame): A pandas DataFrame containing 'text' and LIWC feature columns.
        model_path (str, optional): The path to the saved model file. Defaults to "trained_model.pkl".

    Returns:
        pd.DataFrame: A DataFrame containing predicted personality trait scores and user IDs.
    """

    # Load the saved model
    with open(model_path, 'rb') as file:
        model = pickle.load(file)

    # Tokenize the text column
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    tokens = tokenizer(
        df['text'].fillna("").astype(str).tolist(),
        padding='max_length',
        truncation=True,
        max_length=SEQUENCE_LENGTH,
        return_tensors='np'
    )['input_ids']

    # Select LIWC features (excluding specified columns)
    liwc_columns = [col for col in df.columns if col not in ['userid', 'text', 'words', 'age', 'gender', 'Seg'] + TRAIT_NAMES]
    linguistic_features = df[liwc_columns].values

    # Make predictions
    predictions = model.predict([tokens, linguistic_features])
    predictions = np.array(predictions).squeeze().T  # Reshape predictions

    # Rescale predictions to original range
    unscaled_predictions = np.zeros_like(predictions)
    scaling_factors = {  # Assuming these values are known
        'ope': {'min': 1.0, 'scale': 5.0},
        'con': {'min': 1.25, 'scale': 5.0},
        'ext': {'min': 1.0, 'scale': 5.0},
        'agr': {'min': 1.0, 'scale': 5.0},
        'neu': {'min': 1.0, 'scale': 5.0}
    }

    for i, trait in enumerate(TRAIT_NAMES):
        unscaled_predictions[:, i] = predictions[:, i] * scaling_factors[trait]['scale'] + scaling_factors[trait]['min']

    # Clip predictions to the range [1.0, 5.0]
    unscaled_predictions = np.clip(unscaled_predictions, 1.0, 5.0)

    # Create a DataFrame for results
    results = pd.DataFrame(unscaled_predictions, columns=TRAIT_NAMES)
    results['userid'] = df['userid']

    return results
# def main():
#     # File paths
#     # Load data
#     CLEANED_DATA_PATH = "cleaned.csv"
#     LIWC_PROFILE_PATH = "LIWC.csv"

#     # Load and preprocess data
#     tokens, scaled_ling, y_scaled, scaling_factors = load_and_preprocess(CLEANED_DATA_PATH, LIWC_PROFILE_PATH)

#     logging.info("Data Preprocessing Completed.")

#     # Train and evaluate the model
#     fold_metrics = train_and_evaluate(tokens, scaled_ling, y_scaled, scaling_factors)

#     print("======================================================================")
#     print_metrics(fold_metrics)

# if __name__ == "__main__":
#     main()

from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import numpy as np
import pandas as pd
import pickle
best_models = {}
def train_models(tokens, scaled_ling, y_scaled, scaling_factors, model_type='linear'):
    """
    Trains and evaluates different models for each trait using text tokens and linguistic features.

    Args:
        model_type: str, one of 'linear', 'random_forest', or 'logistic'
    """
    # Combine tokenized data and linguistic features
    combined_features = np.hstack([tokens, scaled_ling])

    kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=RANDOM_STATE)
    fold_metrics = {trait: {'r2': [], 'mae': [], 'rmse': [], 'pearson': []} for trait in TRAIT_NAMES}

    # Model selection
    def get_model(model_type):
        if model_type == 'linear':
            return LinearRegression()
        elif model_type == 'random_forest':
            return RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE)
        elif model_type == 'logistic':
            return LogisticRegression(random_state=RANDOM_STATE, max_iter=1000, solver = 'saga')
        else:
            raise ValueError(f"Unknown model type: {model_type}")

    for fold, (train_idx, val_idx) in enumerate(kf.split(combined_features)):
        print(f"Fold {fold + 1}/{NUM_FOLDS}")

        X_train, X_val = combined_features[train_idx], combined_features[val_idx]
        y_train, y_val = y_scaled[train_idx], y_scaled[val_idx]

        for i, trait in enumerate(TRAIT_NAMES):
            # Initialize model
            model = get_model(model_type)

            # For logistic regression, we need to convert the target to classes
            if model_type == 'logistic':
                # Convert continuous values to binary classes using median as threshold
                threshold = np.median(y_train[:, i])
                y_train_binary = (y_train[:, i] > threshold).astype(int)
                y_val_binary = (y_val[:, i] > threshold).astype(int)

                model.fit(X_train, y_train_binary)
                predictions = model.predict_proba(X_val)[:, 1]  # Get probability scores
            else:
                model.fit(X_train, y_train[:, i])
                predictions = model.predict(X_val)

            # Rescale predictions and ground truth
            predictions_rescaled = predictions * scaling_factors[trait]['scale'] + scaling_factors[trait]['min']
            actual_rescaled = y_val[:, i] * scaling_factors[trait]['scale'] + scaling_factors[trait]['min']

            # Calculate metrics
            r2 = r2_score(actual_rescaled, predictions_rescaled)
            mae = mean_absolute_error(actual_rescaled, predictions_rescaled)
            rmse = np.sqrt(mean_squared_error(actual_rescaled, predictions_rescaled))
            pearson_corr, _ = pearsonr(actual_rescaled, predictions_rescaled)
            fold_metrics[trait]['pearson'].append(pearson_corr)
            fold_metrics[trait]['r2'].append(r2)
            fold_metrics[trait]['mae'].append(mae)
            fold_metrics[trait]['rmse'].append(rmse)

            print(f"{trait.upper()} | R²: {r2:.4f} | MAE: {mae:.4f} | RMSE: {rmse:.4f}")

            # For Random Forest, we can also print feature importance
            if model_type == 'random_forest':
                feature_importance = model.feature_importances_
                print(f"Top 5 most important features for {trait}:")
                # You'll need to create feature names based on your actual features
                feature_names = [f"feature_{i}" for i in range(len(feature_importance))]
                top_features = sorted(zip(feature_importance, feature_names), reverse=True)[:5]
                for importance, name in top_features:
                    print(f"{name}: {importance:.4f}")
            if trait not in best_models or fold_metrics[trait]['mae'][-1] < best_models[trait]['mae']:
                best_models[trait] = {'model': model, 'mae': fold_metrics[trait]['mae'][-1]}
            for trait, model_data in best_models.items():
              with open(f'{model_type}_{trait}_best.pkl', 'wb') as f:
                  pickle.dump(model_data['model'], f)


    return fold_metrics

def main():
    CLEANED_DATA_PATH = "cleaned.csv"
    LIWC_PROFILE_PATH = "LIWC.csv"

    # Load and preprocess data
    tokens, scaled_ling, y_scaled, scaling_factors = load_and_preprocess(CLEANED_DATA_PATH, LIWC_PROFILE_PATH)
    print("Data Preprocessing Completed.")

    # Train and evaluate different models
    models = ['linear','random_forest', 'logistic']

    for model_type in models:
        print(f"\nTraining {model_type.upper()} model:")
        print("=" * 50)
        fold_metrics = train_models(tokens, scaled_ling, y_scaled, scaling_factors, model_type=model_type)
        print(f"\n{model_type.upper()} Model Results:")
        print_metrics(fold_metrics)

if __name__ == "__main__":
    main()